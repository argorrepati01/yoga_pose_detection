{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954e627-f5c9-45e0-802f-0dfa5cef0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import scipy.io\n",
    "from math import ceil, floor\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy import signal\n",
    "from scipy.signal import hilbert, medfilt, butter, lfilter, filtfilt, find_peaks, savgol_filter, welch, find_peaks\n",
    "import numpy as np\n",
    "from math import sqrt, atan2\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, classification_report,ConfusionMatrixDisplay,precision_score, recall_score, f1_score, roc_auc_score,roc_curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9ea2b-b845-4700-99fb-13abe5c80d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting amp and phase values\n",
    "def extract_amp_phase(filename):\n",
    "    \n",
    "    d = pd.read_csv(filename)\n",
    "    col = [i for i in range(192)]\n",
    "    df = pd.DataFrame(columns = col)\n",
    "\n",
    "    amps = []\n",
    "    phases = []\n",
    "    #for 'data' column\n",
    "    for i, j in enumerate(d['data']):\n",
    "\n",
    "        imaginary = []\n",
    "        real = []\n",
    "     \n",
    "        amp= []\n",
    "        ph = []\n",
    "        row= []\n",
    "        #regular expression check for array\n",
    "        csi_string = re.findall(r\"\\[(.*)\\]\", j)[0]\n",
    "        csi_raw = [int(x) for x in csi_string.split(\",\") if x != '']\n",
    "\n",
    "        for k in range(len(csi_raw)):\n",
    "            if (k%2 == 0):\n",
    "                imaginary.append(csi_raw[k])\n",
    "            else:\n",
    "                real.append(csi_raw[k])\n",
    "                \n",
    "        #amplitude and phase extraction\n",
    "        for k in range(int(len(csi_raw) / 2)):\n",
    "            amp.append(round(sqrt(imaginary[k] ** 2 + real[k] ** 2), 1))\n",
    "            ph.append(round(atan2(imaginary[k], real[k])))\n",
    "\n",
    "        amps.append(amp)\n",
    "        phases.append(ph)\n",
    "        \n",
    "    llft_amp = []\n",
    "    llft_phase = []\n",
    "    ht_amp = []\n",
    "    ht_phase = []\n",
    "    #seperating llft amp and phase (26+26 = 52 sc)\n",
    "    llft_amp = np.array([x[6:32] + x[33:59] for x in amps])\n",
    "    llft_phase = np.array([x[6:32] + x[33:59] for x in phases])\n",
    "\n",
    "    #seperating ht amps and phase (57+57 = 114 sc)\n",
    "    ht_amp = np.array([x[66:123] + x[134:191] for x in amps])\n",
    "    ht_phase = np.array([x[66:123] + x[134:191] for x in phases])\n",
    "\n",
    "    return llft_amp, llft_phase, ht_amp, ht_phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af12215-b633-4a62-8c39-6b7e13c4e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hampel filter\n",
    "def hampel_filter(csi_data):\n",
    "    # Define the parameters for the Hampel filter\n",
    "    window_size = 9  # Window size for the median filter\n",
    "    threshold = 2.0  # Threshold for outlier detection (in standard deviations)\n",
    "\n",
    "    # Apply the Hampel filter to each subcarrier (array) separately\n",
    "    hampel_filtered_data = np.zeros_like(csi_data)\n",
    "    for i in range(csi_data.shape[1]):\n",
    "        median_filtered = medfilt(csi_data[:, i], kernel_size=window_size)\n",
    "        deviation = np.abs(csi_data[:, i] - median_filtered)\n",
    "        mad = np.median(deviation)\n",
    "        outliers = deviation > threshold * mad\n",
    "        hampel_filtered_data[:, i] = np.where(outliers, median_filtered, csi_data[:, i])\n",
    "    \n",
    "    return hampel_filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8097ca-7251-4800-95fb-b710245a0873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sg_filter(csi_data):\n",
    "    window_length = 5\n",
    "    polyorder = 3\n",
    "    sg_filter_data = np.zeros_like(csi_data)\n",
    "    for i in range(csi_data.shape[1]):\n",
    "        sg_filter_data[:,i] = savgol_filter(csi_data[:,i], window_length, polyorder)\n",
    "        \n",
    "    return sg_filter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09a129-3496-4b1d-be0e-a046de8e86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered(amp):\n",
    "    filter_hampel = hampel_filter(amp)\n",
    "    filter_sg = sg_filter(filter_hampel)\n",
    "    return filter_sg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de80f1-da43-43bb-b893-cc046c187dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe(folder_path, count, label):\n",
    "   \n",
    "    col = [i for i in range(469)] # since 52 subcarriers and 9 features 52*9 + label = 469\n",
    "    df = pd.DataFrame(columns=col)\n",
    "    \n",
    "    files = [file for file in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file))]\n",
    "    for i, _ in enumerate(range(count)):\n",
    "        data = []\n",
    "        train_data = []\n",
    "        file_name = files[i]\n",
    "        full_file_name = os.path.join(folder_path, file_name)\n",
    "        if os.path.exists(full_file_name):\n",
    "\n",
    "            llft_amp, llft_phase, hft_amp, hft_phase = extract_amp_phase(full_file_name)\n",
    "            amplitude_data = filtered_data(llft_amp)\n",
    "            x_min = np.min(amplitude_data, axis=0)\n",
    "            x_max = np.max(amplitude_data, axis=0)\n",
    "            x_mean = np.mean(amplitude_data, axis=0)\n",
    "            x_variance = np.var(amplitude_data, axis=0)\n",
    "            x_kurtosis = kurtosis(amplitude_data, axis=0)\n",
    "            x_skew = skew(amplitude_data, axis=0)\n",
    "            q1 = np.percentile(amplitude_data, 25, axis=0)\n",
    "            q3 = np.percentile(amplitude_data, 75, axis=0)\n",
    "            x_iqr = q3 - q1\n",
    "            x_rms = np.sqrt(np.mean(amplitude_data**2, axis=0))\n",
    "            x_mean_abs = np.mean(np.abs(amplitude_data), axis=0)\n",
    "            \n",
    "            train_data.append(x_min)\n",
    "            train_data.append(x_max)\n",
    "            train_data.append(x_mean)\n",
    "            train_data.append(x_variance)\n",
    "            train_data.append(x_iqr)\n",
    "            train_data.append(x_rms)\n",
    "            train_data.append(x_skew)\n",
    "            train_data.append(x_kurtosis)\n",
    "            train_data.append(x_mean_abs)\n",
    "            train_data = np.append(train_data, label)\n",
    "           \n",
    "            data.append(train_data)\n",
    "          \n",
    "            df = pd.concat([df, pd.DataFrame(data, columns=col)], ignore_index=True)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb65c0d-cdff-419b-b540-22479a38f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    s1_a = fe(\"aug_yoga_data/a/s1_a_\", \".csv\", 600, 0)\n",
    "    trained_a = pd.concat([s1_a])\n",
    "    \n",
    "    s1_b = fe(\"aug_yoga_data/b/s1_b_\", \".csv\", 600, 1)\n",
    "    trained_b = pd.concat([s1_b])\n",
    "    \n",
    "    s1_c = fe(\"aug_yoga_data/c/s1_c_\", \".csv\", 600, 2)\n",
    "    trained_c = pd.concat([s1_c])\n",
    "    \n",
    "    s1_d = fe(\"aug_yoga_data/d/s1_d_\", \".csv\", 600, 3)\n",
    "    trained_d = pd.concat([s1_d])\n",
    "\n",
    "    s1_e = fe(\"aug_yoga_data/e/s1_e_\", \".csv\", 600, 4)\n",
    "    trained_e = pd.concat([s1_e])\n",
    "\n",
    "\n",
    "    return trained_a, trained_b, trained_c, trained_d, trained_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dea5889-da88-4bb0-a0bb-84b3a651566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.concat([trained_a, trained_b, trained_c, trained_d, trained_e], ignore_index=True)\n",
    "data_set[468] = data_set[468].astype(int)\n",
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aedc14-d84a-49fc-b492-6551af979352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "estimator= RandomForestClassifier()\n",
    "selector = RFE(estimator, n_features_to_select=150, step=1) \n",
    "selector = selector.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943e462-ee6d-4a34-9acb-289a1f515342",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train  = X_train[:, selector.support_]\n",
    "X_test= X_test[:, selector.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b004cf-6a90-48f5-a85e-8d8133a18bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "models={\n",
    "#     \"Logisitic Regression\":LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boost\": GradientBoostingClassifier(),\n",
    "#     \"Adaboost\":AdaBoostClassifier(),\n",
    "    \"Xgboost\": XGBClassifier(),\n",
    "#     \"SVC\": SVC(),\n",
    "    \"KNN\": KNeighborsClassifier()\n",
    "}\n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_train, y_train) # Train model\n",
    "\n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Training set performance\n",
    "    model_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
    "    model_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
    "    model_train_precision = precision_score(y_train, y_train_pred,average='micro') # Calculate Precision\n",
    "    model_train_recall = recall_score(y_train, y_train_pred, average='micro') # Calculate Recall\n",
    "#     model_train_rocauc_score = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "    # Test set performance\n",
    "    model_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
    "    model_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
    "    model_test_precision = precision_score(y_test, y_test_pred, average='micro') # Calculate Precision\n",
    "    model_test_recall = recall_score(y_test, y_test_pred, average='micro') # Calculate Recall\n",
    "#     model_test_rocauc_score = roc_auc_score(y_test, y_test_pred) #Calculate Roc\n",
    "\n",
    "\n",
    "    print(list(models.keys())[i])\n",
    "    \n",
    "    print('Model performance for Training set')\n",
    "    print(\"- Accuracy: {:.4f}\".format(model_train_accuracy))\n",
    "    print('- F1 score: {:.4f}'.format(model_train_f1))\n",
    "    \n",
    "    print('- Precision: {:.4f}'.format(model_train_precision))\n",
    "    print('- Recall: {:.4f}'.format(model_train_recall))\n",
    "#     print('- Roc Auc Score: {:.4f}'.format(model_train_rocauc_score))\n",
    "\n",
    "  \n",
    "    print('----------------------------------')\n",
    "    \n",
    "    print('Model performance for Test set')\n",
    "    print('- Accuracy: {:.4f}'.format(model_test_accuracy))\n",
    "    print('- F1 score: {:.4f}'.format(model_test_f1))\n",
    "    print('- Precision: {:.4f}'.format(model_test_precision))\n",
    "    print('- Recall: {:.4f}'.format(model_test_recall))\n",
    "#     print('- Roc Auc Score: {:.4f}'.format(model_test_rocauc_score))\n",
    "\n",
    "    \n",
    "    print('='*35)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d168e022-47bc-4df3-8eb7-747d9a5591b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter Training\n",
    "rf_params = {\"max_depth\": [5, 8, 15, None, 10],\n",
    "             \"max_features\": [5, 7, \"auto\", 8],\n",
    "             \"min_samples_split\": [2, 8, 15, 20],\n",
    "             \"n_estimators\": [100, 200, 500, 1000]}\n",
    "xgboost_params = {\"learning_rate\": [0.1, 0.01],\n",
    "                  \"max_depth\": [5, 8, 12, 20, 30],\n",
    "                  \"n_estimators\": [100, 200, 300],\n",
    "                  \"colsample_bytree\": [0.5, 0.8, 1, 0.3, 0.4]}\n",
    "gradient_params={\"loss\": ['log_loss','deviance','exponential'],\n",
    "             \"criterion\": ['friedman_mse','squared_error','mse'],\n",
    "             \"min_samples_split\": [2, 8, 15, 20],\n",
    "             \"n_estimators\": [100, 200, 500],\n",
    "              \"max_depth\": [5, 8, 15, None, 10]\n",
    "                }\n",
    "\n",
    "# Models list for Hyperparameter tuning\n",
    "randomcv_models = [(\"RF\", RandomForestClassifier(), rf_params),\n",
    "    (\"XGBoost\", XGBClassifier(), xgboost_params),\n",
    "    (\"GradientBoost\", GradientBoostingClassifier(), gradient_params)\n",
    "                   ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47acd9d-97ce-4e1d-8519-24e139aa56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model_param = {}\n",
    "for name, model, params in randomcv_models:\n",
    "    random = RandomizedSearchCV(estimator=model,\n",
    "                                   param_distributions=params,\n",
    "                                   n_iter=100,\n",
    "                                   cv=3,\n",
    "                                   verbose=2,\n",
    "                                   n_jobs=-1)\n",
    "    random.fit(X_train, y_train)\n",
    "    model_param[name] = random.best_params_\n",
    "\n",
    "for model_name in model_param:\n",
    "    print(f\"---------------- Best Params for {model_name} -------------------\")\n",
    "    print(model_param[model_name])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a3261-9543-4fde-95ca-6715ad6c9cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the values with best params\n",
    "models={\n",
    "    \n",
    "    \"Random Forest\":RandomForestClassifier(n_estimators=500,min_samples_split=2,max_features=8,max_depth=8),\n",
    "    \"XGBClassifier\":XGBClassifier(learning_rate=0.1, max_depth=5, n_estimators=100,colsample_bytree=0.3),\n",
    "    \"GradientBoost\":GradientBoostingClassifier(n_estimators=500, min_samples_split=20, max_depth=15,\n",
    "                                               loss='log_loss', criterion='squared_error')\n",
    "}\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_train, y_train) # Train model\n",
    "\n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Training set performance\n",
    "    \n",
    "    model_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
    "    model_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
    "    model_train_precision = precision_score(y_train, y_train_pred, average='micro') # Calculate Precision\n",
    "    model_train_recall = recall_score(y_train, y_train_pred,average='micro') # Calculate Recall\n",
    "#     model_train_rocauc_score = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "    # Test set performance\n",
    "    model_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
    "    model_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
    "    model_test_precision = precision_score(y_test, y_test_pred, average='micro') # Calculate Precision\n",
    "    model_test_recall = recall_score(y_test, y_test_pred,average='micro') # Calculate Recall\n",
    "#     model_test_rocauc_score = roc_auc_score(y_test, y_test_pred) #Calculate Roc\n",
    "\n",
    "    print(list(models.keys())[i])\n",
    "    \n",
    "    print('Model performance for Training set')\n",
    "    print(\"- Accuracy: {:.4f}\".format(model_train_accuracy))\n",
    "    print('- F1 score: {:.4f}'.format(model_train_f1))\n",
    "    \n",
    "    print('- Precision: {:.4f}'.format(model_train_precision))\n",
    "    print('- Recall: {:.4f}'.format(model_train_recall))\n",
    "#     print('- Roc Auc Score: {:.4f}'.format(model_train_rocauc_score))\n",
    "\n",
    "    \n",
    "    \n",
    "    print('----------------------------------')\n",
    "    \n",
    "    print('Model performance for Test set')\n",
    "    print('- Accuracy: {:.4f}'.format(model_test_accuracy))\n",
    "    print('- F1 score: {:.4f}'.format(model_test_f1))\n",
    "    print('- Precision: {:.4f}'.format(model_test_precision))\n",
    "    print('- Recall: {:.4f}'.format(model_test_recall))\n",
    "#     print('- Roc Auc Score: {:.4f}'.format(model_test_rocauc_score))\n",
    "    cm = confusion_matrix(y_test, y_test_pred, labels=model.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "#     disp.savefig('cm_'+model.classes_+'.png')\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print('='*35)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d89f63-3741-4ca6-8aee-d4cdbad0bb64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
